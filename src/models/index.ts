const modelsTemplate = [
  {
    name: "gpt-3.5-turbo",
    provider: "devchat",
    stream: true,
    max_input_tokens: 13000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "gpt-4",
    provider: "openai",
    stream: true,
    max_input_tokens: 6000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "gpt-4-turbo-preview",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 32000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "claude-2.1",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 32000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "xinghuo-3.5",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 6000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "GLM-4",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 8000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "ERNIE-Bot-4.0",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 8000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "togetherai/codellama/CodeLlama-70b-Instruct-hf",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 4000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "togetherai/mistralai/Mixtral-8x7B-Instruct-v0.1",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 4000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "minimax/abab6-chat",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 4000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
  {
    name: "llama-2-70b-chat",
    provider: "devchat",
    stream: true,
    // max_input_tokens: 4000,
    // temperature: 0.3,
    // max_tokens: 2048,
  },
];

export default modelsTemplate;
